{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intersection of mrcanavar, PacBio CCS, and ONT high coverage regions to identify potential CNV\n",
    "\n",
    "Generating excessive coverage bed files using calculations from mosdepth for both PacBio CCS 15kb_20kb merged and ONT bam files.\n",
    "\n",
    "ultra-long-ont_hs37d5_phased_reheader.bam is from ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/AshkenazimTrio/HG002_NA24385_son/Ultralong_OxfordNanopore/guppy-V2.3.4_2019-06-26/ultra-long-ont_hs37d5_phased.bam\n",
    "\n",
    "HG002.SequelII.merged_15kb_20kb.pbmm2.hs37d5.haplotag.RTG.10x.trio.bam is from ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/AshkenazimTrio/HG002_NA24385_son/PacBio_CCS_15kb_20kb_chemistry2/GRCh37/HG002.SequelII.merged_15kb_20kb.pbmm2.hs37d5.haplotag.RTG.10x.trio.bam\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samtools view -1 -F 0x100 ultra-long-ont_hs37d5_phased_reheader.bam -h > filtered_ultra-long-ont_hs37d5_phased_reheader.bam\n",
    "\n",
    "mosdepth -b 1000 -x --no-per-base HG002.SequelII.merged_15kb_20kb_1000_window_size HG002.SequelII.merged_15kb_20kb.pbmm2.hs37d5.haplotag.RTG.10x.trio.bam\n",
    "\n",
    "mosdepth -b 1000 -x --no-per-base HG002_GRCh37.ultra_long_ONT_1000_window_size filtered_ultra-long-ont_hs37d5_phased_complete.bam\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "chr_1_22 <- seq(1,22)\n",
    "mosdepth_CCS_15kb_20kb_merged_1000_window_size = read.delim(\"HG002.SequelII.merged_15kb_20kb_1000_window_size.regions.bed\", col.names = c(\"CHR\",\"START\",\"END\",\"DEPTH\"))\n",
    "mosdepth_CCS_15kb_20kb_merged_1000_window_size_chr_1_22 <- mosdepth_CCS_15kb_20kb_merged_1000_window_size[which(mosdepth_CCS_15kb_20kb_merged_1000_window_size[,\"CHR\"] %in% chr_1_22),]\n",
    "quantile(mosdepth_CCS_15kb_20kb_merged_1000_window_size_chr_1_22[,\"DEPTH\"]): \n",
    "\n",
    "      0%      25%      50%      75%     100% \n",
    "\n",
    "    0.00    48.12    55.21    61.68 26903.95 \n",
    "\n",
    "IQR(mosdepth_CCS_15kb_20kb_merged_1000_window_size_chr_1_22[,\"DEPTH\"]): 13.56\n",
    "\n",
    "(quantile(mosdepth_CCS_15kb_20kb_merged_1000_window_size_chr_1_22[,\"DEPTH\"])[3]/2)*2.5 : 69.0125\n",
    "\n",
    "\n",
    "mosdepth_ONT_1000_window_size = read.delim(\"HG002_GRCh37.ultra_long_ONT_1000_window_size.regions.bed\", col.names = c(\"CHR\",\"START\",\"END\",\"DEPTH\"))\n",
    "mosdepth_ONT_1000_window_size_chr_1_22 <- mosdepth_ONT_1000_window_size[which(mosdepth_ONT_1000_window_size[,\"CHR\"] %in% chr_1_22),]\n",
    "quantile(mosdepth_ONT_1000_window_size_chr_1_22[,\"DEPTH\"])\n",
    "\n",
    "    0%      25%      50%      75%     100% \n",
    "\n",
    "    0.00    52.36    59.13    65.46 20550.08 \n",
    "\n",
    "IQR(mosdepth_ONT_1000_window_size_chr_1_22[,\"DEPTH\"]): 13.1\n",
    "\n",
    "(quantile(mosdepth_ONT_1000_window_size_chr_1_22[,\"DEPTH\"])[3]/2)*2.5: 73.9125 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "python convert_mosdepth_to_excessive_coverage.py --input HG002_GRCh37_CCS_merged_15kb_20kb_1000_window_size.regions.bed --output HG002_GRCh37_CCS_merged_15kb_20kb_1000_window_size.regions_excessive_coverage_cnv_threshold.bed --threshold 69.0125\n",
    "\n",
    "bedtools intersect -a AJtrio-HG002.hs37d5.300x.bam.bilkentuniv.052119.dups.bed -b HG002_GRCh37_CCS_merged_15kb_20kb_1000_window_size.regions_excessive_coverage_cnv_threshold.bed > mrcanavar_intersect_ccs_cnv_threshold.bed\n",
    "\n",
    "python convert_mosdepth_to_excessive_coverage.py --input HG002_GRCh37.ultra_long_ONT_1000_window_size.regions.bed --output HG002_GRCh37.ultra_long_ONT_1000_window_size.regions_excessive_coverage_cnv_threshold.bed --threshold 73.9125\n",
    "\n",
    "bedtools intersect -a mrcanavar_intersect_ccs_1000_window_size_cnv_threshold.bed -b HG002_GRCh37.ultra_long_ONT_1000_window_size.regions_excessive_coverage_cnv_threshold.bed > mrcanavar_intersect_ccs_1000_window_size_cnv_threshold_intersect_ont_1000_window_size_cnv_threshold.bed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "convert_mosdepth_to_excessive_coverage.py \n",
    "\n",
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser(description=\"Subset bed file to callable regions only\")\n",
    "parser.add_argument('--input_file', metavar=\"I\", type=str, nargs=\"+\", help=\"input bed file\")\n",
    "parser.add_argument('--output_file', metavar=\"O\", type=str, nargs=\"+\", help=\"output file\")\n",
    "parser.add_argument('--threshold', metavar=\"T\", type=str, nargs=\"+\", help=\"input threshold\")\n",
    "args = parser.parse_args()\n",
    "\n",
    "f = open(args.input_file[0], \"r\") \n",
    "f_lines = f.readlines()\n",
    "\n",
    "f_out = open(args.output_file[0], \"w+\")\n",
    "threshold = float(args.threshold[0])\n",
    "\n",
    "for line in f_lines:   \n",
    "    if \"DEPTH\" in line: \n",
    "        continue\n",
    "    line_split = line.split(\"\\t\")\n",
    "    depth_field = float(line_split[3])\n",
    "    if depth_field > threshold:\n",
    "        f_out.write(line)\n",
    "        f_out.flush()  \n",
    "\n",
    "f.close()\n",
    "f_out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Union of CCS and ONT CNV outlier coverage difficult bed to calculate quantiles and IQR for CCS and ONT\n",
    "\n",
    "mosdepth_CCS_15kb_20kb_merged_1000_window_size = read.delim(\"HG002_GRCh37_CCS_merged_15kb_20kb_1000_window_size.regions.bed\", col.names = c(\"CHR\",\"START\",\"END\",\"DEPTH\"))\n",
    "\n",
    "mosdepth_ONT_1000_window_size = read.delim(\"HG002_GRCh37.ultra_long_ONT_1000_window_size.regions.bed\", col.names = c(\"CHR\",\"START\",\"END\",\"DEPTH\"))\n",
    "\n",
    "chr_1_22 <- seq(1,22)\n",
    "mosdepth_CCS_15kb_20kb_merged_1000_window_size_chr_1_22 <- mosdepth_CCS_15kb_20kb_merged_1000_window_size[which(mosdepth_CCS_15kb_20kb_merged_1000_window_size[,\"CHR\"] %in% chr_1_22),]\n",
    "\n",
    "mosdepth_ONT_1000_window_size_chr_1_22 <- mosdepth_ONT_1000_window_size[which(mosdepth_ONT_1000_window_size[,\"CHR\"] %in% chr_1_22),]\n",
    "\n",
    "df_mosdepth_CCS_15kb_20kb_merged_1000_window_size <- data.frame(mosdepth_CCS_15kb_20kb_merged_1000_window_size_chr_1_22)\n",
    "df_mosdepth_ONT_1000_window_size <- data.frame(mosdepth_ONT_1000_window_size_chr_1_22)\n",
    "\n",
    "df_mosdepth_CCS_15kb_20kb_merged_1000_window_size_ONT_1000_combined <- df_mosdepth_CCS_15kb_20kb_merged_1000_window_size\n",
    "\n",
    "df_mosdepth_CCS_15kb_20kb_merged_1000_window_size_ONT_1000_combined[,5] <-  mosdepth_ONT_1000_window_size_chr_1_22[,4]\n",
    "\n",
    "colnames(df_mosdepth_CCS_15kb_20kb_merged_1000_window_size_ONT_1000_combined) <- c(\"CHR\", \"START\", \"END\", \"CCS_DEPTH\", \"ONT_DEPTH\")\n",
    "\n",
    "df_mosdepth_CCS_15kb_20kb_merged_1000_window_size_ONT_1000_combined_elliptical_values <- df_mosdepth_CCS_15kb_20kb_merged_1000_window_size_ONT_1000_combined\n",
    "medianccsdepth = median(mosdepth_CCS_15kb_20kb_merged_1000_window_size_chr_1_22[,\"DEPTH\"])\n",
    "medianontdepth = median(mosdepth_ONT_1000_window_size_chr_1_22[,\"DEPTH\"])\n",
    "df_mosdepth_CCS_15kb_20kb_merged_1000_window_size_ONT_1000_combined_elliptical_values[,6] <- sqrt(((df_mosdepth_CCS_15kb_20kb_merged_1000_window_size_ONT_1000_combined[,4]/medianccsdepth)^2 + (df_mosdepth_CCS_15kb_20kb_merged_1000_window_size_ONT_1000_combined[,5]/medianontdepth)^2)/2)\n",
    "threshold_ellipctial_outlier = unname(quantile(df_mosdepth_CCS_15kb_20kb_merged_1000_window_size_ONT_1000_combined_elliptical_values[,6])[4]+(1.5*IQR(df_mosdepth_CCS_15kb_20kb_merged_1000_window_size_ONT_1000_combined_elliptical_values[,6])[1]))\n",
    "\n",
    "df_mosdepth_CCS_15kb_20kb_merged_1000_window_size_ONT_1000_combined_outliers <- df_mosdepth_CCS_15kb_20kb_merged_1000_window_size_ONT_1000_combined_elliptical_values[which(df_mosdepth_CCS_15kb_20kb_merged_1000_window_size_ONT_1000_combined_elliptical_values[,6] > 1.360939),]\n",
    "\n",
    "write.csv(df_mosdepth_CCS_15kb_20kb_merged_1000_window_size_ONT_1000_combined_outliers, file = \"union_HG002_CCS_15kb_20kb_merged_ONT_1000_window_size_combined_elliptical_outlier_threshold.bed\", row.names = FALSE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alignment of assemblies to identify potential CNV\n",
    "\n",
    "Nancy Hansen provided an both assembly coverage bedgraphs of maternal and paternal haplotypes in December 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HG2_SKor_TrioONTCanu\n",
    "unionBedGraphs -i HG2_SKor_TrioONTCanuMat_1119_l100c500.qdelta.sort.renamedcontigs.bedgraph HG2_SKor_TrioONTCanuPat_1119_l100c500.qdelta.sort.renamedcontigs.bedgraph > HG2_SKor_TrioONTCanu_1119_l100c500.qdelta.sort.renamedcontigs.tempunion.bedgraph\n",
    "\n",
    "python find_union.py --input HG2_SKor_TrioONTCanu_1119_l100c500.qdelta.sort.renamedcontigs.tempunion.bedgraph --output HG2_SKor_TrioONTCanu_1119_l100c500.union.qdelta.sort.renamedcontigs.bedgraph\n",
    "\n",
    "python remove_all_but_chr1_22.py --input HG2_SKor_TrioONTCanu_1119_l100c500.union.qdelta.sort.renamedcontigs.bedgraph  --output HG2_SKor_TrioONTCanu_1119_l100c500.union.qdelta.sort.renamedcontigs.chr1_22.bedgraph\n",
    "\n",
    "# HG2_SKor_TrioONTFlye\n",
    "unionBedGraphs -i HG2_SKor_TrioONTFlyeMat_1119_l100c500.qdelta.sort.renamedcontigs.bedgraph HG2_SKor_TrioONTFlyePat_1119_l100c500.qdelta.sort.renamedcontigs.bedgraph > HG2_SKor_TrioONTFlye_1119_l100c500.qdelta.sort.renamedcontigs.tempunion.bedgraph\n",
    "\n",
    "python find_union.py --input HG2_SKor_TrioONTFlye_1119_l100c500.qdelta.sort.renamedcontigs.tempunion.bedgraph --output HG2_SKor_TrioONTFlye_1119_l100c500.union.qdelta.sort.renamedcontigs.bedgraph\n",
    "\n",
    "python remove_all_but_chr1_22.py --input HG2_SKor_TrioONTFlye_1119_l100c500.union.qdelta.sort.renamedcontigs.bedgraph --output HG2_SKor_TrioONTFlye_1119_l100c500.union.qdelta.sort.renamedcontigs.chr1_22.bedgraph\n",
    "\n",
    "# HG2_SKor_CCS15\n",
    "unionBedGraphs -i HG2_SKor_CCS15Mat_0918_l100c500.qdelta.sort.renamedcontigs.bedgraph HG2_SKor_CCS15Pat_0918_l100c500.qdelta.sort.renamedcontigs.bedgraph > HG2_SKor_CCS15_0918_l100c500.qdelta.sort.renamedcontigs.tempunion.bedgraph\n",
    "\n",
    "python find_union.py --input HG2_SKor_CCS15_0918_l100c500.qdelta.sort.renamedcontigs.tempunion.bedgraph --output HG2_SKor_CCS15_0918_l100c500.union.qdelta.sort.renamedcontigs.bedgraph\n",
    "\n",
    "python remove_all_but_chr1_22.py --input HG2_SKor_CCS15_0918_l100c500.union.qdelta.sort.renamedcontigs.bedgraph --output HG2_SKor_CCS15_0918_l100c500.union.qdelta.sort.renamedcontigs.chr1_22.bedgraph\n",
    "\n",
    "## Intersect and find > 10kb\n",
    "\n",
    "intersectBed -wa -a HG2_SKor_TrioONTCanu_1119_l100c500.union.qdelta.sort.renamedcontigs.chr1_22.bedgraph -b HG2_SKor_CCS15_0918_l100c500.union.qdelta.sort.renamedcontigs.chr1_22.bedgraph | sort -k1,1 -k2,2n - | bedtools intersect -a stdin -b HG2_SKor_TrioONTFlye_1119_l100c500.union.qdelta.sort.renamedcontigs.chr1_22.bedgraph | sort -k1,1 -k2,2n - | mergeBed -i stdin -d 100  | awk '$3-$2>10000' > HG2_SKor_TrioONTCanu_intersect_HG2_SKor_TrioONTFlye_intersect_HG2_SKor_CCS15_gt10kb_GRCh37.bed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "find_union.py\n",
    "\n",
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser(description=\"Subset bed file to >1 for column 4\")\n",
    "parser.add_argument('--input', metavar=\"I\", type=str, nargs=\"+\", help=\"input bed file\")\n",
    "parser.add_argument('--output', metavar=\"O\", type=str, nargs=\"+\", help=\"output file\")\n",
    "args = parser.parse_args()\n",
    "\n",
    "f = open(args.input[0], \"r\") \n",
    "f_lines = f.readlines()\n",
    "\n",
    "f_out = open(args.output[0], \"w+\")\n",
    "number_entries_covered = 0\n",
    "for line in f_lines:\n",
    "    split_line = line.split(\"\\t\")\n",
    "    if int(split_line[3].strip()) > 1 or int(split_line[4].strip()) > 1:\n",
    "        number_entries_covered = number_entries_covered + int(split_line[3])\n",
    "        f_out.write(line)\n",
    "        f_out.flush()\n",
    "\n",
    "f.close()\n",
    "f_out.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "remove_all_but_chr1_22.py \n",
    "\n",
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser(description=\"Remove all but chr1-22 entries\")\n",
    "parser.add_argument('--input', metavar=\"I\", type=str, nargs=\"+\", help=\"input file\")\n",
    "parser.add_argument('--output', metavar=\"O\", type=str, nargs=\"+\", help=\"output file\")\n",
    "args = parser.parse_args()\n",
    "\n",
    "f = open(args.input[0], \"r\") \n",
    "f_lines = f.readlines()\n",
    "chrs_to_keep = [\"chr1\",\"chr2\",\"chr3\",\"chr4\",\"chr5\",\"chr6\",\"chr7\",\"chr8\",\"chr9\",\"chr10\",\"chr11\",\"chr12\",\"chr13\",\"chr14\",\"chr15\",\"chr16\",\"chr17\",\"chr18\",\"chr19\",\"chr20\",\"chr21\",\"chr22\"]\n",
    "\n",
    "f_out = open(args.output[0], \"w+\")\n",
    "\n",
    "for line in f_lines:\n",
    "    split_line = line.split(\"\\t\")\n",
    "    if split_line[0] in chrs_to_keep:\n",
    "        f_out.write(line)\n",
    "        f_out.flush()\n",
    "\n",
    "f.close()\n",
    "f_out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inversions \n",
    "\n",
    "Nancy Hansen provided a list of inversions that were identified in HG002 in December 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bedtools slop -i hsd37d5.112719.inversions.bed -g human.b37.genome -b 0.25 -pct > hsd37d5.112719.inversions_slop150.bed\n",
    "\n",
    "python remove_all_but_chr1_22.py --input hsd37d5.112719.inversions_slop150.bed --output hsd37d5.112719.inversions_slop150_chr1_22.bed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "convert_inversions_vcf_to_bed.py\n",
    "\n",
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser(description=\"Convert inversions VCF to bed\")\n",
    "parser.add_argument('--input', metavar=\"I\", type=str, nargs=\"+\", help=\"input bed file\")\n",
    "parser.add_argument('--output', metavar=\"O\", type=str, nargs=\"+\", help=\"output bed file\")\n",
    "args = parser.parse_args()\n",
    "\n",
    "f = open(args.input[0], \"r\") \n",
    "f_out = open(args.output[0], \"w+\")\n",
    "\n",
    "lines = f.readlines()\n",
    "\n",
    "for line in lines:\n",
    "    if \"#\" in line:\n",
    "        continue\n",
    "    split_line = line.split(\"\\t\")\n",
    "    info_field = split_line[7]\n",
    "    info_field_split = info_field.split(\";\")\n",
    "    chrom = \"0\"\n",
    "    start = \"0\"\n",
    "    end = \"0\"\n",
    "    clusterIDs = \"\"\n",
    "    for split in info_field_split:\n",
    "        if \"REFWIDENED\" in split:\n",
    "            refwidened_split = split.split(\"=\")\n",
    "            refwidened_split_on_colon = refwidened_split[1].split(\":\")\n",
    "            chrom = refwidened_split_on_colon[0]\n",
    "            refwidened_split_start_end = refwidened_split_on_colon[1].split(\"-\")\n",
    "            start = refwidened_split_start_end[0]\n",
    "            end = refwidened_split_start_end[1]\n",
    "        elif \"ClusterIDs\" in split:\n",
    "            clusterIDs = split\n",
    "    to_write_out = chrom + \"\\t\" + start + \"\\t\" + end + \"\\t\" + clusterIDs + \"\\n\"\n",
    "    f_out.write(to_write_out)\n",
    "    f_out.flush()\n",
    "\n",
    "f.close()\n",
    "f_out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Aaron's VDJ regions file\n",
    "\n",
    "Aaron Wenger's produced a bed file (hg19.vdj.bed.gz) that covers the VDJ and the README file he provided includes commands for generating it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find L1Hs greater than 500 base pairs\n",
    "\n",
    "Lines 1-3 are from Aaron Wenger's manual curation BEDs hg19 README."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#download file from https://hgdownload.cse.ucsc.edu/goldenPath/hg19/database/rmsk.txt.gz\n",
    "\n",
    "cat rmsk.txt | awk '{ print $6 \"\\t\" $7 \"\\t\" $8 \"\\t\" $12 \":\" $11 \"\\t\" $10; }' > hg19.rmsk.bed\n",
    "\n",
    "grep \"L1H\" hg19.rmsk.bed > hg19.rmsk.L1H.bed\n",
    "\n",
    "python filter_size_gt.py --input hg19.rmsk.L1H.bed --output hg19.rmsk.L1H_gt500.bed --filter 500\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "filter_size_gt.py\n",
    "\n",
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser(description=\"Filter regions based on size\")\n",
    "parser.add_argument('--input', metavar=\"I\", type=str, nargs=\"+\", help=\"input file\")\n",
    "parser.add_argument('--output', metavar=\"O\", type=str, nargs=\"+\", help=\"output file\")\n",
    "parser.add_argument('--filter', metavar=\"F\", type=str, nargs=\"+\", help=\"filter file\")\n",
    "args = parser.parse_args()\n",
    "\n",
    "f = open(args.input[0], \"r\") \n",
    "f_lines = f.readlines()\n",
    "\n",
    "f_out = open(args.output[0], \"w+\")\n",
    "size_filter = int(args.filter[0])\n",
    "for line in f_lines:\n",
    "    line_split = line.split(\"\\t\")\n",
    "    start = int(line_split[1])\n",
    "    end = int(line_split[2])\n",
    "    size = end - start\n",
    "    if size > size_filter:         \n",
    "        f_out.write(line)\n",
    "        f_out.flush()\n",
    "\n",
    "    \n",
    "f.close()\n",
    "f_out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate segdups_chr1_22_gte_10kb_identity_gte_990_segdups_counts_gt_5.bed\n",
    "\n",
    "Line 1 of these commands was adopted from Aaron Wenger's scripts to generate manual curation BED files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#download file from http://hgdownload.cse.ucsc.edu/goldenPath/hg19/database/genomicSuperDups.txt.gz\n",
    "\n",
    "python remove_all_but_chr1_22_w_chr.py --input hg19.segdups.bed --output hg19.segdups_chr1_22.bed\n",
    "\n",
    "python filter_segdup_size_bed.py --input hg19.segdups_chr1_22.bed --output1 hg19.segdups_chr1_22_gte_10kb.bed --output2 hg19.segdups_chr1_22_lt_10kb.bed --filter 10000  \n",
    "\n",
    "python filter_percent_identity_bed.py --input hg19.segdups_chr1_22_gte_10kb.bed --output1 hg19.segdups_chr1_22_gte_10kb_identity_gte_990.bed --output2 hg19.segdups_chr1_22_gte_10kb_identity_lt_990.bed --filter 990    \n",
    "\n",
    "sort -k 1,1 hg19.segdups_chr1_22_gte_10kb_identity_gte_990.bed > hg19.segdups_chr1_22_gte_10kb_identity_gte_990.sorted_for_genomecov.bed\n",
    "\n",
    "bedtools genomecov -bg -i hg19.segdups_chr1_22_gte_10kb_identity_gte_990.sorted_for_genomecov.bed -g GRCh37.genome > hg19.segdups_chr1_22_gte_10kb_identity_gte_990_segdups_counts.bed\n",
    "\n",
    "python filter_count_gt_bed.py --input hg19.segdups_chr1_22_gte_10kb_identity_gte_990_segdups_counts.bed --output hg19.segdups_chr1_22_gte_10kb_identity_gte_990_segdups_counts_gt_5.bed --filter 5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "filter_segdup_size_bed.py\n",
    "\n",
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser(description=\"Filter segdups based on size\")\n",
    "parser.add_argument('--input', metavar=\"I\", type=str, nargs=\"+\", help=\"input file\")\n",
    "parser.add_argument('--output1', metavar=\"O\", type=str, nargs=\"+\", help=\"output file 1\")\n",
    "parser.add_argument('--output2', metavar=\"U\", type=str, nargs=\"+\", help=\"output file 2\")\n",
    "parser.add_argument('--filter', metavar=\"F\", type=str, nargs=\"+\", help=\"filter file\")\n",
    "args = parser.parse_args()\n",
    "\n",
    "f = open(args.input[0], \"r\") \n",
    "f_lines = f.readlines()\n",
    "\n",
    "f_out1 = open(args.output1[0], \"w+\")\n",
    "f_out2 = open(args.output2[0], \"w+\")\n",
    "size_filter = int(args.filter[0])\n",
    "for line in f_lines:\n",
    "    line_split = line.split(\"\\t\")\n",
    "    segdup_start = int(line_split[1])\n",
    "    segdup_end = int(line_split[2])\n",
    "    segdup_size = segdup_end - segdup_start\n",
    "    if segdup_size >= size_filter:         \n",
    "        f_out1.write(line)\n",
    "        f_out1.flush()\n",
    "    elif segdup_size < size_filter:\n",
    "        f_out2.write(line)\n",
    "        f_out2.flush()\n",
    "    \n",
    "f.close()\n",
    "f_out1.close()\n",
    "f_out2.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "filter_percent_identity_bed.py\n",
    "\n",
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser(description=\"Filter segdups based on percent identity\")\n",
    "parser.add_argument('--input', metavar=\"I\", type=str, nargs=\"+\", help=\"input file\")\n",
    "parser.add_argument('--output1', metavar=\"O\", type=str, nargs=\"+\", help=\"output file 1\")\n",
    "parser.add_argument('--output2', metavar=\"U\", type=str, nargs=\"+\", help=\"output file 2\")\n",
    "parser.add_argument('--filter', metavar=\"F\", type=str, nargs=\"+\", help=\"filter file\")\n",
    "args = parser.parse_args()\n",
    "\n",
    "f = open(args.input[0], \"r\") \n",
    "f_lines = f.readlines()\n",
    "\n",
    "f_out1 = open(args.output1[0], \"w+\")\n",
    "f_out2 = open(args.output2[0], \"w+\")\n",
    "size_filter = int(args.filter[0])\n",
    "for line in f_lines:\n",
    "    line_split = line.split(\"\\t\")\n",
    "    segdup_percent_identity = int(line_split[5])\n",
    "    if segdup_percent_identity >= size_filter:         \n",
    "        f_out1.write(line)\n",
    "        f_out1.flush()\n",
    "    elif segdup_percent_identity < size_filter:\n",
    "        f_out2.write(line)\n",
    "        f_out2.flush()\n",
    "    \n",
    "f.close()\n",
    "f_out1.close()\n",
    "f_out2.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "filter_count_gt_bed.py\n",
    "\n",
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser(description=\"Filter regions by count\")\n",
    "parser.add_argument('--input', metavar=\"I\", type=str, nargs=\"+\", help=\"input file\")\n",
    "parser.add_argument('--output', metavar=\"O\", type=str, nargs=\"+\", help=\"output file\")\n",
    "parser.add_argument('--filter', metavar=\"F\", type=str, nargs=\"+\", help=\"filter file\")\n",
    "args = parser.parse_args()\n",
    "\n",
    "f = open(args.input[0], \"r\") \n",
    "f_lines = f.readlines()\n",
    "\n",
    "f_out = open(args.output[0], \"w+\")\n",
    "size_filter = int(args.filter[0])\n",
    "for line in f_lines:\n",
    "    line_split = line.split(\"\\t\")\n",
    "    segdup_count = int(line_split[3])\n",
    "    if segdup_count > size_filter:         \n",
    "        f_out.write(line)\n",
    "        f_out.flush()\n",
    "\n",
    "    \n",
    "f.close()\n",
    "f_out1.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expand gaps by 15,000 base pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bedtools slop -i example_of_no_ref_regions_input_file_b37.bed -g genome.dict -b 15000 | mergeBed -i stdin > example_of_no_ref_regions_input_file_b37_bedtools_slop_15kb_merged.bed\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expand GIAB SV v0.6 by 50%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bedtools slop -i HG002_SVs_Tier1plusTier2_v0.6.1.bed -g genome.dict -b .25 -pct | mergeBed -i stdin > expanded_150_HG002_SVs_Tier1plusTier2_v0.6.1.bed\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regions covered by no contig in 38 to 37 alignment and regions covered by more than 1 contigs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate coverage of genome regions by 38 to 37 alignment\n",
    "/Volumes/DroboZook/bioinfo/bedtools2.25.0/bin/genomeCoverageBed -ibam /Volumes/giab/data/reference_genomes/refrefalignments/hs37d5_hg38_minimap2_asm20_N10.bam -bg -g /Volumes/DroboZook/bioinfo/bedtools2.25.0/genomes/human.b37.genome.txt > /Volumes/giab/data/reference_genomes/refrefalignments/hs37d5_hg38_minimap2_asm20_N10_coverage.bed\n",
    "#regions larger than 1kb, after merging gaps with 100bp, covered by >1 GRCh38 contig\n",
    "awk '$4>1' /Volumes/giab/data/reference_genomes/refrefalignments/hs37d5_hg38_minimap2_asm20_N10_coverage.bed | /Volumes/DroboZook/bioinfo/bedtools2.25.0/bin/mergeBed -i stdin -d 100 | awk '$3-$2>1000' > /Volumes/giab/data/reference_genomes/refrefalignments/hs37d5_hg38_minimap2_asm20_N10_gt1contig_gt1kb.bed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find assembly contigs less than 500kb\n",
    "\n",
    "Lines 1-7 are from Aaron Wenger's analysis of singly unique nucleotides in v4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download gap track from ftp://hgdownload.cse.ucsc.edu/goldenPath/hg19/database/gap.txt.gz ; convert to BED\n",
    "cat gap.txt | cut -f2-4,8 | awk '{n=$4; if(n==\"clone\" || n==\"contig\" || n==\"scaffold\" || n==\"short_arm\") { n=\"gap\"; } print $1\"\\t\"$2\"\\t\"$3\"\\t\"n; }' | sort -k1,1 -k2,2g > hg19.gap.bed\n",
    "\n",
    "# complement to identify contigs; ignore random chromosomes. ignoring 'alt' and 'fix' are new for GRCh37. Create BED with score set to contig length.\n",
    "grep -v -e 'hap' hg19.gap.bed > hg19.gap_no_random.bed\n",
    "cat genome.fa.fai | sort -k1,1 > hg19_sorted.fa.fai \n",
    "bedtools complement -g hg19_sorted.fa.fai -i hg19.gap_no_random.bed | awk '($1>=1 && $1<=22) { print $0 \"\\tcontig\\t\" $3-$2; }' > hg19.contigs.1-22.bed\n",
    "\n",
    "awk '($5 < 5e5)' hg19.contigs.1-22.bed > hg19.contigs.1-22.lt_500kb.bed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SP_SPonly regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Valerie Schnieder pointed out there are GRCh37 alignments to GRCh38 with regions resolved and the type. \n",
    "\n",
    "Information can be found at NCBI Assm-Assm alignments explained https://www.ncbi.nlm.nih.gov/genome/tools/remap/docs/alignments\n",
    "\n",
    "The GRCh37 aligned to GRCh38 (excluding alts): ftp://ftp.ncbi.nlm.nih.gov/pub/remap/Homo_sapiens/2.1/GCF_000001405.13_GRCh37/GCF_000001405.26_GRCh38/\n",
    "\n",
    "For the differences between the files these types were of interest: Discrepancy = SP : regions that are collapsed Discrepancy = SP-only : regions that are expanded\n",
    "\n",
    "At that ftp link, the file GCF_000001405.13.xlxs was parsed to create a bed file containing SP and SP-only regions named SP_SPonly_primary_assm_alignments_sorted_merged.bed"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bash",
   "language": "bash",
   "name": "bash"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
